{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bc3f599",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "48003d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446235ff",
   "metadata": {},
   "source": [
    "todo: initialise constants from the dataset (drop name column)\n",
    "todo: redefine feed forward generally\n",
    "todo: do todo "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e1bc6f",
   "metadata": {},
   "source": [
    "Intialise network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7ba24926",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 3  # number of layers(excluding input layer)\n",
    "n = 2  # number of features\n",
    "m = 1  # number of output nodes\n",
    "\n",
    "hidden_layers = 2  # number of layers(excluding input layer)\n",
    "features = 2  # number of features\n",
    "outputs = 1  # number of output nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1e358a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_network(input_size=features, hidden_size=features+1, output_size=outputs, num_hidden_layers=hidden_layers):\n",
    "    weights = []\n",
    "    biases = []\n",
    "    \n",
    "    # Input to first hidden layer\n",
    "    weights.append(np.random.randn(hidden_size, input_size) * 0.01)\n",
    "    biases.append(np.random.randn(hidden_size, 1) * 0.01)\n",
    "    \n",
    "    # Hidden layers\n",
    "    for i in range(num_hidden_layers - 1):\n",
    "        weights.append(np.random.randn(hidden_size, hidden_size) * 0.01)\n",
    "        biases.append(np.random.randn(hidden_size, 1) * 0.01)\n",
    "    \n",
    "    # Last hidden to output layer\n",
    "    weights.append(np.random.randn(output_size, hidden_size) * 0.01)\n",
    "    biases.append(np.random.randn(output_size, 1) * 0.01)\n",
    "    \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "803fd336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights of the network: [array([[-0.0065811 , -0.00094568],\n",
      "       [ 0.01696198,  0.00922171],\n",
      "       [ 0.00129864, -0.0092376 ]]), array([[ 1.63511434e-02,  1.12045895e-03,  7.05345569e-03],\n",
      "       [ 2.45510363e-05, -7.49741630e-03,  3.09564123e-03],\n",
      "       [ 6.39512349e-03,  3.36422767e-03, -1.52499969e-02]]), array([[-0.01222086,  0.0009136 ,  0.00015016]])]\n",
      "Biases of the network: [array([[0.01409215],\n",
      "       [0.0036405 ],\n",
      "       [0.00611037]]), array([[ 0.00272478],\n",
      "       [-0.01465117],\n",
      "       [-0.00879309]]), array([[0.01159015]])]\n"
     ]
    }
   ],
   "source": [
    "weights, biases = init_network()\n",
    "print(\"Weights of the network:\", weights)\n",
    "print(\"Biases of the network:\", biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c54697",
   "metadata": {},
   "source": [
    "Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b1ce2426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "  X = np.array([\n",
    "      [150, 70],\n",
    "      [254, 73],\n",
    "      [312, 68],\n",
    "      [120, 60],\n",
    "      [154, 61],\n",
    "      [212, 65],\n",
    "      [216, 67],\n",
    "      [145, 67],\n",
    "      [184, 64],\n",
    "      [130, 69]\n",
    "  ])\n",
    "  y = np.array([0,1,1,0,0,1,1,0,1,0])\n",
    "  m = y.shape[0]  # number of samples\n",
    "  # Transpose the input matrix to match the expected shape\n",
    "  A0 = X.T\n",
    "  # Reshape y to a matrix\n",
    "  Y = y.reshape(outputs, m)\n",
    "\n",
    "  return A0, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aa1882b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61ed4b5",
   "metadata": {},
   "source": [
    "Activation function sigmoid \n",
    "$$g(z)=\\frac{1}{(1+e^{-z})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d161c1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation function\n",
    "def sigmoid(arr):\n",
    "    return 1 / (1+np.exp(-1*arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69c4b8e",
   "metadata": {},
   "source": [
    "Feed forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fdd8be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(input_layer=x_train, num_hidden_layers=hidden_layers, weights=weights, biases=biases):\n",
    "  cache = [input_layer]\n",
    "  \n",
    "  for i in range(0,num_hidden_layers+1):\n",
    "    prev_layer = cache[i]\n",
    "    W = weights[i]\n",
    "    b = biases[i]\n",
    "    Z = W @ prev_layer + b\n",
    "    A = sigmoid(Z)\n",
    "    cache.append(A)\n",
    "\n",
    "  return cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "84444ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions of the network: [[0.50149336 0.50149455 0.50149491 0.5014928  0.50149328 0.50149403\n",
      "  0.5014941  0.50149325 0.5014937  0.50149307]]\n",
      "\n",
      "Predictions: [array([[150, 254, 312, 120, 154, 212, 216, 145, 184, 130],\n",
      "       [ 70,  73,  68,  60,  61,  65,  67,  67,  64,  69]]), array([[0.26129076, 0.1510304 , 0.10875538, 0.30314191, 0.25786712,\n",
      "        0.1911476 , 0.18682313, 0.26824833, 0.22142972, 0.287675  ],\n",
      "       [0.96058735, 0.99320783, 0.99732961, 0.93036965, 0.96000986,\n",
      "        0.9852113 , 0.98641732, 0.95610147, 0.97621982, 0.94505948],\n",
      "       [0.39037734, 0.41620278, 0.44599345, 0.40316148, 0.41159069,\n",
      "        0.42091388, 0.41768022, 0.39543883, 0.41431704, 0.38640206]]), array([[0.50270672, 0.50231069, 0.50219156, 0.50289187, 0.50272997,\n",
      "        0.50248074, 0.5024577 , 0.50274283, 0.50259037, 0.50280321],\n",
      "       [0.49484063, 0.4947988 , 0.49481387, 0.49490741, 0.49485811,\n",
      "        0.49481768, 0.49481289, 0.494853  , 0.49482961, 0.49486682],\n",
      "       [0.49753909, 0.49729179, 0.4971141 , 0.49753185, 0.49745226,\n",
      "        0.49733124, 0.49733767, 0.49752714, 0.49739724, 0.49758337]]), array([[0.50149336, 0.50149455, 0.50149491, 0.5014928 , 0.50149328,\n",
      "        0.50149403, 0.5014941 , 0.50149325, 0.5014937 , 0.50149307]])]\n"
     ]
    }
   ],
   "source": [
    "cache = feed_forward(x_train)  # Get the output layer predictions\n",
    "predictions = cache[-1]  # Last layer output\n",
    "print(\"Predictions of the network:\", cache[-1])\n",
    "print(\"\\nPredictions:\", cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bb48ba",
   "metadata": {},
   "source": [
    "Use loss as cost function: $L\\left(\\hat{y}_i y_i\\right)=-\\left(y_i \\ln \\hat{y}_i+\\left(1-y_i\\right) \\ln \\left(1-\\hat{y}_i\\right)\\right)$\n",
    "\n",
    "\n",
    "Hence cost given by: $C=\\frac{1}{m} \\sum_{i=1}^m L\\left(\\hat{y}_i, y_i\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "24f5db76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(y_hat=predictions, y=y_train):\n",
    "  \"\"\"\n",
    "  y_hat should be a n^L x m matrix\n",
    "  y should be a n^L x m matrix\n",
    "  \"\"\"\n",
    "\n",
    "  # Clip predictions to avoid log(0)\n",
    "  y_hat = np.clip(y_hat, 1e-15, 1 - 1e-15)\n",
    "\n",
    "  # 1. losses is a n^L x m\n",
    "  losses = - ( (y * np.log(y_hat)) + (1 - y)*np.log(1 - y_hat) )\n",
    "\n",
    "  return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "acb06e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931505366938915\n"
     ]
    }
   ],
   "source": [
    "print(cost(predictions, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0125e8c",
   "metadata": {},
   "source": [
    "Backpropagation with formulas derived using calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8843ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_layer(dC_dA_current, A_current, A_prev, W_current, Y, layer_num):\n",
    "    \"\"\"\n",
    "    Backpropagation for a single layer.\n",
    "\n",
    "    Args:\n",
    "        dC_dA_current:  Propagator from the next layer (dC/dA[l+1]). For the output layer, this is None.\n",
    "        A_current:      Activations of the current layer (A[l])\n",
    "        A_prev:         Activations of the previous layer (A[l-1])\n",
    "        W_current:      Weights of the current layer (W[l])\n",
    "        Y:              True labels (only needed for the output layer)\n",
    "\n",
    "    Returns:\n",
    "        dC_dW: Gradient of cost with respect to weights\n",
    "        dC_db: Gradient of cost with respect to biases\n",
    "        dC_dA_prev: Propagator to the previous layer (dC/dA[l-1])\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate dC/dZ\n",
    "    if dC_dA_current is None:  # Output layer\n",
    "        dC_dZ = (1/m) * (A_current - Y)\n",
    "    else:  # Hidden layer\n",
    "        dA_dZ = A_current * (1 - A_current)\n",
    "        dC_dZ = dC_dA_current * dA_dZ\n",
    "\n",
    "\n",
    "    # calculate dC/dW\n",
    "    dZ_dW = A_prev\n",
    "    dC_dW = dC_dZ @ dZ_dW.T\n",
    "\n",
    "    # step 3. calculate dC/db\n",
    "    dC_db = np.sum(dC_dZ, axis=1, keepdims=True)\n",
    "\n",
    "    # calculate propagator to the prev layer\n",
    "    if layer_num != 1:\n",
    "        dZ_dA_prev = W_current\n",
    "        dC_dA_prev = W_current.T @ dC_dZ\n",
    "\n",
    "    else:\n",
    "        dC_dA_prev = None\n",
    "\n",
    "    return dC_dW, dC_db, dC_dA_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18478c6",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e6629794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=200, alpha=0.1, hidden_layers=hidden_layers, features=features, outputs=outputs):\n",
    "  global weights, biases, x_train, y_train\n",
    "  \n",
    "  costs = []\n",
    "\n",
    "  for e in range(epochs):\n",
    "    # Feed forward\n",
    "    cache = feed_forward(x_train)\n",
    "    predictions = cache[-1]\n",
    "    \n",
    "    # Cost\n",
    "    costs.append(cost(predictions, y_train))\n",
    "\n",
    "    # Backpropagation \n",
    "    updates = []\n",
    "    for layer_num in range(hidden_layers, 0, -1):\n",
    "        if layer_num == hidden_layers:\n",
    "            # For the output layer, we need to pass the true labels Y\n",
    "            propagations = backprop_layer(\n",
    "                dC_dA_current=None, \n",
    "                A_current=cache[layer_num], \n",
    "                A_prev=cache[layer_num-1], \n",
    "                W_current=weights[layer_num], \n",
    "                Y=y_train,\n",
    "                layer_num=layer_num\n",
    "            )\n",
    "        else:\n",
    "            # For hidden layers, we pass the propagator from the next layer\n",
    "            propagations = backprop_layer(\n",
    "                dC_dA_current=updates[0], \n",
    "                A_current=cache[layer_num], \n",
    "                A_prev=cache[layer_num-1], \n",
    "                W_current=weights[layer_num],\n",
    "                layer_num=layer_num\n",
    "            )\n",
    "\n",
    "        updates.insert(0, propagations)\n",
    "\n",
    "    # Update weights and biases\n",
    "    for layer_num in range(hidden_layers, -1, -1):\n",
    "        dC_dW, dC_db, dC_dA_prev = updates[layer_num]\n",
    "\n",
    "        # Update weights and biases\n",
    "        weights[layer_num] -= alpha * dC_dW\n",
    "        biases[layer_num] -= alpha * dC_db\n",
    "\n",
    "\n",
    "    if e % 20 == 0:\n",
    "      print(f\"epoch {e}: cost = {costs[e]:4f}\")\n",
    "  \n",
    "  return costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e85edc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,1,-1):\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a652bb19",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m costs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[73], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epochs, alpha, hidden_layers, features, outputs)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(hidden_layers, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m layer_num \u001b[38;5;241m==\u001b[39m hidden_layers:\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;66;03m# For the output layer, we need to pass the true labels Y\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m         propagations \u001b[38;5;241m=\u001b[39m \u001b[43mbackprop_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdC_dA_current\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43mA_current\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_num\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43mA_prev\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_num\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43mW_current\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer_num\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m            \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlayer_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_num\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;66;03m# For hidden layers, we pass the propagator from the next layer\u001b[39;00m\n\u001b[0;32m     29\u001b[0m         propagations \u001b[38;5;241m=\u001b[39m backprop_layer(\n\u001b[0;32m     30\u001b[0m             dC_dA_current\u001b[38;5;241m=\u001b[39mupdates[\u001b[38;5;241m0\u001b[39m], \n\u001b[0;32m     31\u001b[0m             A_current\u001b[38;5;241m=\u001b[39mcache[layer_num], \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m             layer_num\u001b[38;5;241m=\u001b[39mlayer_num\n\u001b[0;32m     35\u001b[0m         )\n",
      "Cell \u001b[1;32mIn[72], line 36\u001b[0m, in \u001b[0;36mbackprop_layer\u001b[1;34m(dC_dA_current, A_current, A_prev, W_current, Y, layer_num)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layer_num \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     35\u001b[0m     dZ_dA_prev \u001b[38;5;241m=\u001b[39m W_current\n\u001b[1;32m---> 36\u001b[0m     dC_dA_prev \u001b[38;5;241m=\u001b[39m \u001b[43mW_current\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdC_dZ\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     dC_dA_prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 1)"
     ]
    }
   ],
   "source": [
    "costs = train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
